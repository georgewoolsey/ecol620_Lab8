---
title: "Lab 8 - Ecological Niche Modeling"
subtitle: "ECOL 620 - Applications in Landscape Ecology"
author: "George Woolsey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
    # code_folding: hide
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding){ 
    out_dir <- '../';
    rmarkdown::render(inputFile, encoding = encoding, output_file=file.path(dirname(inputFile), out_dir, 'index.html'));
    file.copy(from = "..//index.html", to = '..///data//lab8_george_woolsey.html', overwrite = TRUE)
  })
---

# Setup

```{r, include=FALSE, warning=F, message=F}
# knit options
knitr::opts_chunk$set(
  echo = TRUE
  , warning = FALSE
  , message = FALSE
  # , results='hide'
  , fig.width = 10
  , fig.height = 7
)
```

```{r}
# bread-and-butter
library(tidyverse)
library(lubridate)
library(viridis)
library(scales)
library(latex2exp)
# visualization
library(kableExtra)
library(cowplot)
library(RColorBrewer)
library(ggtext) # color text on ggplot
# spatial analysis
library(terra)
# ecological niche modeling
library(mgcv)             #for gams
library(randomForest)     #for random forest SDMs
library(MuMIn)            #for model selection
library(PresenceAbsence)  #for model evaluation
library(ecospat)         #for model evaluation
library(dismo)            #for SDMs... requires `rJava`
# :'(  :'(  :'(  :'(  :'(  :'(  :'(  
library(raster)
library(sp)
# set seed
set.seed(11)
```

## Rules

1. Using the provided R Markdown template, answer all questions and show your R code where necessary. Note, some questions are just plain text written answers.

2. Complete your assignment using the R markdown file and submit individual assignments to Canvas. Knit your script and submit an .html file on Canvas. Please use  the following naming convention: lab8_firstname_lastname.html (ex. lab8_kyle_horton.html).  Note, we will not grade labs in any other format.

```{r, include=FALSE, eval=FALSE}
corraltive or mechanistic niche modelling?
  correlative models (most common) ... using in this lab

types of response data: presence only, presence-absence data (preferred)
generating "absences": presence only data can be used in isolation or they can be compared to background points, sometimes called "pseudo absences" to build distribtion models...would like to see distribution of environmental variable to differ between presence obs and "pseudo absence" obs

```

---

# Question 1

For the five models used in this exercise (envelope, GAM, GLM, Random Forest, and Maxent), list a pro AND con of each approach. (10 pts)

1. **Generalized Linear Models (GLMs)**
    i. **Pro:** widely used for distribution modeling, allows for alternative distributions for the response variable other than the normal distribution including Poisson, binomial, Bernoulli, and gamma distributions
    ii. **Con:** Although adding quadratic and cubic terms allows for some potential non-linear responses, GLMs have limited capacity to capture non-linear response functions which are often highlighted in niche theory
2. **Generalized Additive Models (GAMs)**
    i. **Pro:** can accommodate non-linearity through the use of “smoothers” that attempt to generalize data into smooth curves by local fitting to subsections of the data (e.g. running average, LOWESS)
    ii. **Con:** do not allow for parameter estimation (e.g. slope) of the nonlinear relationship
3. **Random Forest**
    i. **Pro:** have high predictive performance with reduced bias, and reduced variance in estimates. Can accommodate non-linear relationships and interactions, insensitive to outliers, and can accommodate missing data in predictor variables
    ii. **Con:** Interpretation of large "forests" which are comprised of many classification trees can be difficult
4. **Maximum Entropy (Maxent)**
    i. **Pro:** widely used for distribution modeling, particularly well suited for presence-only data since it does not assume background points are absences as in GAMs, GLMs, and regression trees
    ii. **Con:** underlying assumptions make it unsuitable for presence–absence analysis of species distributions

---

# Data Preparation

## Bird survey data

We will use Varied Thrush (*Ixoreus naevius*), American Ornithological Union (AOU) alpha code **VATH**, surveys from northern Idaho and western Montana to build candidate distribution models.

```{r, results='hide', fig.show='hide'}
# bird survey data
  # load data
  vath_survey <- read.csv(file="../data/vath_2004.csv", header=TRUE) |> 
    dplyr::rename_with(tolower) |> 
    dplyr::mutate(year = 2004)
  vath_validation <- read.csv(file="../data/vath_VALIDATION.csv", header=TRUE) |> 
    dplyr::rename_with(tolower) |> 
    dplyr::rename(
      surveyid = x
      , point = stop
    )
  # view data
  vath_survey |> dplyr::glimpse()
  vath_survey |> dplyr::count(vath)
  vath_validation |> dplyr::glimpse()
  vath_validation |> dplyr::count(vath)
  # combine data
  vath_survey_full <- dplyr::bind_rows(
      vath_survey 
      , vath_validation |> dplyr::select(names(vath_survey))
    ) |> 
    dplyr::mutate(
      year_lab = ifelse(year >= 2007 & year <= 2008, "2007-2008", as.character(year))
    )
  
```

## Environmental covariate data

```{r, results='hide', fig.show='hide'}
# raster environmental variables
  #elevation layer
  elev <- raster::raster("../data/elev.gri") |> terra::rast()
  #linear gradient in canopy cover taken from PCA
  canopy <- raster::raster("../data/cc2.gri") |> terra::rast()
  #presence of mesic forest
  mesic <- raster::raster("../data/mesic.gri") |> terra::rast()
  #mean precip (cm)
  precip <- raster::raster("../data/precip.gri") |> terra::rast()
#compare extents and crs
  terra::ext(elev) == terra::ext(canopy)
  terra::ext(elev) == terra::ext(mesic)
  terra::ext(elev) == terra::ext(precip)
  terra::crs(elev) == terra::crs(canopy)
  terra::crs(elev) == terra::crs(mesic)
  terra::crs(elev) == terra::crs(precip)
# view data
  terra::values(elev) |> hist()
  terra::values(canopy) |> hist()
  terra::values(mesic) |> hist()
  terra::values(precip) |> hist()
#resample to align layers
  #for categorical use nearest neighbor
  mesic <- terra::resample(x=mesic, y=elev, method = "near") |> 
    terra::crop(elev)
  #for continuous data use bilinear
  precip <- resample(x=precip, y=elev, method = "bilinear") |> 
    terra::crop(elev)
#make 1 km wet forest
  fw_temp <- terra::focalMat(mesic, d = 1000, type = "circle")
  mesic1km <- terra::focal(mesic, w = fw_temp, fun = "sum", na.rm=T)
#create raster stack
layers <- c(canopy, elev, mesic, mesic1km, precip)
names(layers) <- c("canopy", "elev", "mesic", "mesic1km", "precip")
layers
```

investigate correlation among environmental covariates

```{r}
terra::pairs(layers, maxcells = min(10000, terra::ncell(layers)*.1))
# remove mesic layer
layers <- terra::subset(layers, subset = c("mesic"), negate = T)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

---

# Question 2

Generate a two-panel map of the study extent that includes the underlying elevation and the presence and absence observations. Include one panel for 2004 and one for the 2007-2008 data. Color points blue for presence and red for absence (see Canvas example figure). Bird fun fact, ornithologists use four letter codes to denote species common names. In this case, Varied Thrush translates to VATH. (6 pts)

**use true presence/absence and not psuedo-absence**

```{r}
# convert survey and validation data to spatial
vath_survey_full <- sf::st_as_sf(
    vath_survey_full
    , coords = c("easting", "northing")
    , crs = terra::crs(elev)
  ) |> 
  dplyr::mutate(
    pres_abs = ifelse(vath == 1, "Present", "Absent") |> as.factor()
  )
vath_survey <- sf::st_as_sf(
    vath_survey
    , coords = c("easting", "northing")
    , crs = terra::crs(elev)
  )
vath_validation <- sf::st_as_sf(
    vath_validation
    , coords = c("easting", "northing")
    , crs = terra::crs(elev)
  )
# plot
ggplot() + 
  geom_raster(
    data = elev |> as.data.frame(xy=T) |> dplyr::rename(elev=3)
    , mapping = aes(x=x,y=y,fill=elev)
  ) +
  geom_sf(data = vath_survey_full, mapping = aes(color = pres_abs), show.legend = F) +
  facet_grid(cols = vars(year_lab)) + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_gradient(low = "black", high = "gray95", na.value = "transparent") +
  scale_color_manual(values = c("firebrick", "navy")) +
  labs(
    x = "Longitude"
    , y = "Latitude"
    , fill = "Elevation (km)"
    , color = "VATH presence"
    , subtitle = "<span><span style='color:FireBrick;'><b><i>VATH Absent</i></b></span> | <span style='color:Navy;'><b><i>VATH Present</i></b></span></span>"
  ) +
  theme_light() +
  theme(
    legend.position = "bottom"
    , legend.direction = "horizontal"
    , legend.text = element_text(size = 8, angle = 25, hjust = 0.7)
    , legend.title = element_text(size = 8)
    , axis.title = element_text(size = 8)
    , axis.text.y = element_text(size=7)
    , axis.text.x = element_text(size=7)
    , panel.grid = element_blank()
    , strip.text = element_text(size = 14, color = "black")
    , strip.background = element_blank()
    , plot.subtitle = ggtext::element_markdown(size = 10)
  )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```


# Question 3

The example code from Fletcher and Fortin is formulated for the use of presence-only information, albeit at times with synthetic absence information. Why do you think they have ignored the true absence information? (3 pts)

<span style="color: teal;">
Fletcher and Fortin note that presence-only data are widely available across broad geographic areas. As such, it is possible that the authors wanted to provide example analysis for this type of data, including the generation of background points for simulating absence data, even though there was absence data available in these survey data. Furthermore, it is sometimes argued that presence-only data can circumvent the problem of false negatives in presence–absence data (i.e., recording an absence when in fact the species is present).
</span>

# Question 4

Using the existing code, build the GLM, GAM, and random forest models using the true presence AND absence information in the “vath_2004.csv”. You can direct the model construction as you see fit. Create a three-panel plot of the predicted occurrence for the three presence-absence models generated (see Canvas example figure). (15 pts)

## Data preparation

Sample background points: Generate random points that can be used to extract background values ("random-absence"). The points are sampled (without replacement) from the cells that are not `NA` in raster

```{r}
#Generate availability/background points using dismo
background_points <- dismo::randomPoints(
    layers |> raster::raster()
    , p = vath_survey |> dplyr::filter(vath == 1) |> sf::st_coordinates()
    , n = 2000
  ) |> 
  dplyr::as_tibble() |> 
  dplyr::mutate(sampid = dplyr::row_number()) |> 
  sf::st_as_sf(
    coords = c("x", "y")
    , crs = terra::crs(elev)
  )
# quick plot
ggplot() + 
  geom_raster(data = layers$elev |> as.data.frame(xy=T), aes(x=x,y=y,fill=elev)) +
  geom_sf(data = background_points, alpha = 0.6) +
  scale_fill_viridis_c() +
  labs(title = "background sample points", fill="elev (km)") +
  theme_light()

```

Extract environmental data at point locations

```{r}
#extract GIS data
#extracts values from layers at pres locations
  pres.cov_temp <- terra::extract(
    layers
    , vath_survey |> 
      dplyr::filter(vath == 1) |> 
      terra::vect()
    , ID = F
  ) |> 
  dplyr::bind_cols(
    vath_survey |> dplyr::filter(vath == 1) |> dplyr::select(geometry)
  ) |> 
  ## make sure no data is missing
  dplyr::filter(dplyr::if_all(dplyr::everything(), ~ !is.na(.x))) |> 
  dplyr::mutate(presence = 1)
#extracts values from layers at random locations
  back.cov_temp <- terra::extract(
    layers
    , background_points |> 
      terra::vect()
    , ID = F
  ) |> 
  dplyr::bind_cols(
    background_points |> dplyr::select(geometry)
  ) |> 
  ## make sure no data is missing
  dplyr::filter(dplyr::if_all(dplyr::everything(), ~ !is.na(.x))) |> 
  dplyr::mutate(presence = 0)
# combine into training data
  training_cover <- rbind(pres.cov_temp, back.cov_temp)
#extracts values from layers at validation locations
  validation_cover <- terra::extract(
    layers
    , vath_validation |> 
      terra::vect()
    , ID = F
  ) |> 
  dplyr::bind_cols(
    vath_validation |> dplyr::mutate(presence = vath) |> dplyr::select(geometry, presence)
  ) |> 
  ## make sure no data is missing
  dplyr::filter(dplyr::if_all(dplyr::everything(), ~ !is.na(.x)))
  

```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Models

### Envelope Model

[Fletcher and Fortin (2018)](https://link.springer.com/book/10.1007/978-3-030-01989-1) p. 240:

*Envelope models can be readily fit in the `dismo` package. In these models, we only use the presence locations. To create the envelope, the `bioclim` function in `dismo` calculates the percentiles of observed environmental covariates at presence locations and the values of covariates at each location on the map are compared to these percentiles. The closer the value of the location to the median value of a covariate at presence locations, the more suitable that location is deemed to be. Then, the minimum similarity value across covariates is used.*

```{r, results='hide', fig.show='hide'}
#fit model
mod_envelope <- dismo::bioclim(
  x = layers |> raster::stack()
  , p = training_cover |> 
      dplyr::filter(presence == 1) |>
      dplyr::pull(geometry) |>
      sf::as_Spatial()
)
#inspect
summary(mod_envelope)
#plot
#elev-canopy plot 85% quantile bounding box
plot(mod_envelope, a=1, b=2, p=0.95)
#elev-canopy plot 95% quantile bounding box
plot(mod_envelope, a=1, b=2, p=0.90)
#elev-precip plot
plot(mod_envelope, a=1, b=4, p=0.95)
# store predictions
pred_envelope <- terra::predict(object = layers, model = mod_envelope)
#plot
plot(pred_envelope)
```

### GLM

```{r, results='hide', fig.show='hide'}
# glm model
mod_glm <- stats::glm(
  formula = presence ~ canopy + elev + I(elev^2) + mesic1km + precip
  , family = binomial(link=logit)
  , data = training_cover |> sf::st_drop_geometry()
)
# store predictions
pred_glm <- terra::predict(object = layers, model = mod_glm, type = "response")
#plot
plot(pred_glm)
```

### GAM

```{r}
# GAM (default settings with optimal knots determined by generalized cross validation)
mod_gam_def <- mgcv::gam(
  formula = presence ~ s(canopy) + s(elev) + s(mesic1km) + s(precip)
  , family = binomial(link=logit)
  , method = "ML"
  , data = training_cover |> sf::st_drop_geometry()
)
#Adjust the number of knots
## knots 3
mod_gam_knt3 <- mgcv::gam(
  formula = presence ~ s(canopy, k = 3) + s(elev, k = 3) + s(mesic1km, k = 3) + s(precip, k = 3)
  , family = binomial(link=logit)
  , method = "ML"
  , data = training_cover |> sf::st_drop_geometry()
)
## knots 6
mod_gam_knt6 <- mgcv::gam(
  formula = presence ~ s(canopy, k = 6) + s(elev, k = 6) + s(mesic1km, k = 6) + s(precip, k = 6)
  , family = binomial(link=logit)
  , method = "ML"
  , data = training_cover |> sf::st_drop_geometry()
)
#plot relationships and compare
# plot(mod_gam_knt3, shade=T)
# plot(mod_gam_knt6, shade=T)
#Consider interactions among splines with tensors (this is slow; ~ 6min)
if(FALSE){
mod_gam_tnsr <- mgcv::gam(
  formula = presence ~ te(canopy, elev, precip, mesic1km)
  , family = binomial(link=logit)
  , method = "ML"
  , data = training_cover |> sf::st_drop_geometry()
)
}
#Change the smoothing function
# use cubic spline basis... see: ?mgcv::smooth.terms
mod_gam_cbcspln <- mgcv::gam(
  formula = presence ~ s(canopy, bs = "cr") + s(elev, bs = "cr") + s(mesic1km, bs = "cr") + s(precip, bs = "cr")
  , family = binomial(link=logit)
  , method = "ML"
  , data = training_cover |> sf::st_drop_geometry()
)
```

#### Evaluate model predictions

```{r, results='asis'}
##
#evaluation with AIC
mod_aic_temp <- round(AIC(mod_gam_def, mod_gam_knt3, mod_gam_knt6, mod_gam_cbcspln), 1) |> 
  as.data.frame() |> 
  tibble::rownames_to_column("model") |> 
  dplyr::arrange(AIC) |> 
  dplyr::mutate(rank = dplyr::row_number()) |> 
  dplyr::relocate(rank)
# table
  kableExtra::kable(
    mod_aic_temp
      , format = "html" 
      , caption = "AIC of GAM models"
      , digits = 3
    ) |> 
    kable_styling(font_size = 14)
```

store predictions

```{r, fig.show='hide'}
# store predictions
pred_gam <- terra::predict(object = layers, model = mod_gam_knt6, type = "response")
#plot
plot(pred_gam)
```

### Random Forest

```{r, results='hide', fig.show='hide'}
#random forest model (default)
mod_rf_def <- randomForest::randomForest(
  formula = as.factor(presence) ~ canopy + elev + mesic1km + precip
  , na.action = na.omit
  , data = training_cover |> sf::st_drop_geometry()
)
#tuning model
rf_tune <- randomForest::tuneRF(
  y = as.factor(training_cover$presence)
  , x = training_cover |> sf::st_drop_geometry() |> dplyr::select(canopy,elev,mesic1km,precip)
  , stepFactor = 0.5
  , ntreeTry = 500
)
#update rf model with mtry=1 based on tuning
mod_rf_def <- randomForest::randomForest(
  formula = as.factor(presence) ~ canopy + elev + mesic1km + precip
  , mtry = 1
  , ntree = 500
  , na.action = na.omit
  , data = training_cover |> sf::st_drop_geometry()
)
#variable importance plot
randomForest::varImpPlot(mod_rf_def)
# store predictions
pred_rf <- terra::predict(object = layers, model = mod_rf_def, type = "prob")[[2]]
#plot
plot(pred_rf)
```

## Model Predictions

```{r}
# plot
ggplot() + 
  geom_raster(
    data = 
      dplyr::bind_rows(
        pred_glm |> as.data.frame(xy=T) |> dplyr::rename(occurance=3) |> dplyr::mutate(model = "GLM")
        , pred_gam |> as.data.frame(xy=T) |> dplyr::rename(occurance=3) |> dplyr::mutate(model = "GAM")
        , pred_rf |> as.data.frame(xy=T) |> dplyr::rename(occurance=3) |> dplyr::mutate(model = "Random Forest")
      ) |> 
      dplyr::mutate(
        model = factor(model, levels = c("GLM", "GAM", "Random Forest"), ordered = T)
      )
    , mapping = aes(x=x,y=y,fill=occurance)
  ) +
  geom_sf(data = vath_survey_full |> dplyr::slice_head(n=1), show.legend = F, color = "transparent") +
  facet_grid(cols = vars(model)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_viridis_c(option = "plasma", na.value = "transparent") +
  labs(
    x = "Longitude"
    , y = "Latitude"
    , fill = "Occurance"
  ) +
  theme_light() +
  theme(
    legend.position = "bottom"
    , legend.direction = "horizontal"
    , legend.text = element_text(size = 8, angle = 25, hjust = 0.7)
    , legend.title = element_text(size = 8)
    , axis.title = element_text(size = 8)
    , axis.text.y = element_text(size=7)
    , axis.text.x = element_text(size=7)
    , panel.grid = element_blank()
    , strip.text = element_text(size = 13, color = "black")
    , strip.background = element_blank()
  )

```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

# Question 5

For these three models, generate the partial dependence plots. If possible, generalize across the models to describe the association of Varied Thrush occurrence with the predictors (see Canvas example figure). (10 pts)

## Partial Response Data Preparation

In *partial response plots* (or “partial plots”), one environmental covariate is varied across the range of observed values while setting all other environmental covariates to a constant, typically their mean or median. Predictions are made on this new data set to interpret how the models are relating species occurrence to environmental factors. Note that this approach will not adequately illuminate potential interactions between variables if they are considered in models (e.g., through the use of tensor products in GAMs or in Random Forest models). See [Fletcher and Fortin (2018)](https://link.springer.com/book/10.1007/978-3-030-01989-1) p. 251

```{r}
# filter training data for background (presence=0) locations only
  # aggregate
back.cov_summary <- training_cover |>
  dplyr::filter(presence==0) |> 
  dplyr::summarise(
    dplyr::across(
      c(elev, canopy, precip, mesic1km)
      , list(
        min = ~ min(.x, na.rm=T)
        , max = ~ max(.x, na.rm=T)
        , median = ~ median(.x, na.rm=T)
      )
    )
  )
# function to generate data frame over range of data holding other covariates constant
partial_response_df_fn <- function(my_var, var_list) {
  #Data frame of data over range of selected var
    seq_temp <- seq(
        from = back.cov_summary |> dplyr::pull(paste0(my_var,"_min"))
        , to = back.cov_summary |> dplyr::pull(paste0(my_var,"_max"))
        , length = 100
      )
    # holding other covariates constant
    cov_df_temp <- data.frame(x = seq_temp) |> 
    dplyr::cross_join(
      back.cov_summary |> 
        dplyr::select(paste0(var_list[var_list!=my_var], "_median"))
    ) |> 
    setNames(c(my_var, var_list[var_list!=my_var])) |>
    dplyr::select(dplyr::all_of(var_list))
  # model predictions based on this data
    pred_df <- data.frame(
      covariate = my_var
      , covariate_value = seq_temp
      , pred_envelope = terra::predict(mod_envelope, cov_df_temp) |> c()
      , pred_glm = terra::predict(mod_glm, cov_df_temp, type="response") |> c()
      , pred_gam = terra::predict(mod_gam_knt6, cov_df_temp, type="response") |> c()
      , pred_rf = terra::predict(mod_rf_def, cov_df_temp, type="prob") |> 
          as.data.frame() |> dplyr::pull(2)
    )
  # return
    return(pred_df)
}
# map over all covariates
var_list_temp <- c("elev", "canopy", "precip", "mesic1km")
var_list_nms <- c("Elevation (km)", "Canopy Cover (linear gradient)", "Precipitation (cm)", "% Mesic Habitat within 1km")
partial_resonse_df <- var_list_temp |> 
  purrr::map(partial_response_df_fn, var_list = var_list_temp) |> 
  dplyr::bind_rows() |> 
  tidyr::pivot_longer(
    cols = tidyselect::starts_with("pred_")
    , names_to = "model"
    , values_to = "predicted_occurance"
    , names_prefix = "pred_"
    , values_drop_na = F
  ) |> 
  dplyr::mutate(
    covariate = factor(
      covariate
      , levels = var_list_temp
      , labels = var_list_nms
      , ordered = T
    )
    , model = factor(
      model
      , levels = c("envelope", "glm", "gam", "rf")
      , labels = c("Envelope", "GLM", "GAM", "Random Forest")
      , ordered = T
    )
  )
```

## Partial Response Plots

```{r}
ggplot(
    data = partial_resonse_df |> dplyr::filter(tolower(model) != "envelope")
    , mapping = aes(
      x = covariate_value
      , y = predicted_occurance
      , color = model
    )
  ) +
  geom_line(linewidth = 1.1) +
  facet_wrap(
    facets = vars(covariate)
    , ncol = 2
    , strip.position = "bottom"
    , scales = "free_x"
  ) +
  scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
  scale_color_viridis_d(option = "turbo", alpha = 0.8) +
  labs(
    x = ""
    , y = "Predicted Occurance"
  ) +
  theme_light() +
  theme(
    legend.position = "top"
    , legend.direction = "horizontal"
    , legend.title = element_blank()
    , axis.title = element_text(size = 8)
    , axis.text = element_text(size=7)
    , strip.background = element_blank()
    , strip.text = element_text(color = "black")
    , strip.placement = "outside"
  ) +
  guides(colour = guide_legend(override.aes = list(size = 10)))
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

# Question 6

Do your models show improvement as compared to the “presence only” models? Be sure you are comparing apples-to-apples regarding the model construction. For instance, does the model you are comparing have the same predictors? Examine the Model Evaluation section for techniques in formalizing your conclusion. (6 pts)

## Data Preparation

[Fletcher and Fortin (2018)](https://link.springer.com/book/10.1007/978-3-030-01989-1) p. 253:

*The `dismo` package includes the evaluation function, but here we use the `PresenceAbsence` package (Freeman and Moisen 2008), which includes a more comprehensive set of evaluation metrics. To use the `PresenceAbsence` package, we create a data frame that includes (in the following order): (1) site IDs for the validation (evaluation) data; (2) the observed responses in the validation data; and (3) model predictions for those locations. This data frame can have predictions from $N$ models, where columns for predictions are $3$ to $N+3$.*

```{r}
#predictions for validation
var_list_temp <- c("elev", "canopy", "precip", "mesic1km")
cov_df_temp <- validation_cover |> dplyr::select(dplyr::all_of(var_list_temp))
# data frame for presence-abscense
# model predictions based on this data
# val.data
validation_pred_df <- data.frame( 
  siteID = 1:nrow(validation_cover)
  , obs = validation_cover$presence
  , pred_envelope = terra::predict(mod_envelope, cov_df_temp) |> c()
  , pred_glm = terra::predict(mod_glm, cov_df_temp, type="response") |> c()
  , pred_gam = terra::predict(mod_gam_knt6, cov_df_temp, type="response") |> c()
  , pred_rf = terra::predict(mod_rf_def, cov_df_temp, type="prob") |> 
      as.data.frame() |> dplyr::pull(2)
)
#correlation among model predictions
cor_matrix <- cor(
    validation_pred_df |> dplyr::select(dplyr::starts_with("pred_"))
    , method="spearman"
  )
  # keep lower triangle
  cor_matrix[!lower.tri(cor_matrix, diag = FALSE)] <- NA
  cor_matrix <- cor_matrix[2:nrow(cor_matrix), 1:ncol(cor_matrix)-1]
# table of correlation matrix
  options(knitr.kable.NA = "")
  kableExtra::kable(cor_matrix
      , format = "html" 
      , caption = "Correlation between model preditions of VATH occurance"
      , digits = 3
    ) |> 
    kable_styling(font_size = 11)
```

## Evaluation of modeling algorithms

[Fletcher and Fortin (2018)](https://link.springer.com/book/10.1007/978-3-030-01989-1) p. 254:

*For model evaluation, we will calculate three continuous metrics: AUC, the biserial correlation coefficient, and the cross-validated log-likelihood (Lawson et al. 2014). We will also calculate four binary metrics taken from the confusion matrix: sensitivity, specificity, kappa, and the true skill statistic. The `PresenceAbsence` package can determine thresholds based on a variety of criteria, such as prevalence in the test or training data, maximizing kappa or maximizing the sum of specificity and sensitivity (see ?optimal.thresholds). Here, we focus on using a threshold that maximizes the sum of specificity and sensitivity (opt.methods¼3 in the optimal.thresholds function), which was recommended by Liu et al. (2013). In the following for loop, we calculate each of these metrics for each model and populate our summary data frame with the output. We first load the `PresenceAbsence` package and detach glmnet, because the latter package also includes a function for calculating AUC.*

```{r}
#data frame to store summary statistics
summary.eval <- data.frame(matrix(nrow=0, ncol=9))
names(summary.eval) <- c("model", "auc", "corr", "ll", "threshold", "sens", "spec", "tss", "kappa")
nmodels <- ncol(validation_pred_df)-2
# loop to generate model validation statistics
for(i in 1:nmodels){
  #calculate summary statistics
  auc.i <- PresenceAbsence::auc(validation_pred_df, which.model=i)
  kappa.opt <- PresenceAbsence::optimal.thresholds(validation_pred_df, which.model=i, opt.methods=3)
  sens.i <- PresenceAbsence::sensitivity(
    PresenceAbsence::cmx(
      validation_pred_df, which.model=i,threshold = kappa.opt[[2]]
    ))
  spec.i <- PresenceAbsence::specificity(
    PresenceAbsence::cmx(
      validation_pred_df, which.model=i,threshold = kappa.opt[[2]]
    ))
  tss.i<- sens.i$sensitivity + spec.i$specificity - 1
  kappa.i <- PresenceAbsence::Kappa(
    PresenceAbsence::cmx(
      validation_pred_df, which.model=i,threshold = kappa.opt[[2]]
    ))
  corr.i <- stats::cor.test(validation_pred_df[,2], validation_pred_df[,i+2])$estimate
  ll.i <- sum(log(validation_pred_df[,i+2]*validation_pred_df[,2] + (1-validation_pred_df[,i+2])*(1-validation_pred_df[,2])))
  ll.i <- ifelse(ll.i=="-Inf", sum(log(validation_pred_df[,i+2]+0.001)*validation_pred_df[,2] + log((1-validation_pred_df[,i+2]))*(1-validation_pred_df[,2])), ll.i)

  #summarize
  summary.i <- c(i,auc.i$AUC, corr.i, ll.i,kappa.opt[[2]], sens.i$sensitivity, spec.i$specificity, tss.i, kappa.i[[1]])
  summary.eval <- rbind(summary.eval, summary.i)
}
names(summary.eval) <- c("model", "auc", "corr", "ll", "threshold", "sens", "spec", "tss", "kappa")
#add model names
summary.eval$model <- validation_pred_df |> dplyr::select(dplyr::starts_with("pred_")) |> names() |> stringr::str_remove("pred_") 
#inspect
summary.eval |> 
  kableExtra::kable(
      format = "html" 
      , caption = "Evaluation of modeling algorithms based on external validation (presence–absence data collected 3–4 years later)"
      , digits = 3
    ) |> 
    kable_styling(font_size = 11)

```

<span style="color: teal;">
The "envelope" model only uses the presence locations. Comparing the other models to the envelope model shows that all models - GLM, GAM, and Random Forest - tended to predict better than the presence-only model based on the Area under the Receiver Operating Characteristic (ROC) Curve (AUC) criterion (an AUC value of 0.5 can be interpreted as the model performing no better than a random prediction). Evaluating model performance based on the True Skill Statistic (TSS) yields similar results with the GLM and GAM models performing better than the envelope and random forest models (a TSS of +1 indicates perfect agreement and values of zero or less indicate a performance no better than random).
</span>
